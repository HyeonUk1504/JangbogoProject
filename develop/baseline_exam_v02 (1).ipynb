{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_exam_v02.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "74ac2c347abb782a3d2f694c8d9341193b1005c179381ae3abf4a67743d7a946"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBmN69pUKCV0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyB18APVX2n5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mGU9KBKQa5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmoGpfz4KhKp"
      },
      "source": [
        "# conda install pytorch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDH8C91VKhm5"
      },
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0PLRGOZKk5Z"
      },
      "source": [
        "seed_everything(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wImxXHPBKmh4"
      },
      "source": [
        "# 데이터 로드\n",
        "data = pd.read_csv('/content/drive/MyDrive/멀캠_장보고/train.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-v6UCCdK7p5"
      },
      "source": [
        "data['요일'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvJN7j6QKq7Q"
      },
      "source": [
        "# 데이터 정규화\n",
        "# 요일을 0 ~ 6으로 변환 후 각 컬럼 최대값으로 나눠 0 ~ 1값으로 정규화 진행\n",
        "\n",
        "week_day_map = {}\n",
        "for i, d in enumerate(data['요일'].unique()):\n",
        "    week_day_map[d] = i\n",
        "data['요일'] = data['요일'].map(week_day_map)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnfjk79rXgtx"
      },
      "source": [
        "# debug\n",
        "# for i, d in enumerate(data['요일'].unique()):\n",
        "#   print(i, d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsC0lYWkMi7x"
      },
      "source": [
        "# 데이터 정규화\n",
        "norm = data.iloc[:, 1:].max(0)\n",
        "data.iloc[:, 1:] = data.iloc[:, 1:] / norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDrWXo2WMkIu"
      },
      "source": [
        "# 하이퍼파라미터\n",
        "device = torch.device(\"cuda:0\") # GPU 사용\n",
        "target_n = 21 # 맞춰야하는 품목/품종의 수\n",
        "learning_rate = 5e-4 # 학습률\n",
        "BATCH_SIZE = 128 # 배치사이즈\n",
        "EPOCHS = 50 # 총 eopochs\n",
        "teacher_forcing = False # 교사강요 설정\n",
        "n_layers = 3 # rnn레이어 층\n",
        "dropout = 0.2 # 드롭아웃\n",
        "window_size = 28 # 인코더 시퀀스 길이\n",
        "future_size = 28 # 디코더 시퀀스 길이\n",
        "hidden_dim = 128 # rnn 히든차원\n",
        "save_path = f'/content/drive/MyDrive/Colab Notebooks/models/best_model.pt' # 모델 저장 경로"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW-fs-_ZOQRq"
      },
      "source": [
        "# pd.set_option('display.max_columns', 200)\n",
        "# row 생략 없이 출력\n",
        "pd.set_option('display.max_rows', None)\n",
        "# col 생략 없이 출력\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zIpkF7hQ6cJ"
      },
      "source": [
        "data.iloc[0:100, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzlqaAEeORZp"
      },
      "source": [
        "x_data1 = data.iloc[0:0+window_size, 1:]\n",
        "x_data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUWXUw3JObcq"
      },
      "source": [
        "x_data2 = data.iloc[1:1+window_size, 1:].head(window_size+11)\n",
        "x_data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd99p7z_Xgtz"
      },
      "source": [
        "# 전처리\n",
        "# 시계열 학습이 가능한 형태로 전처리\n",
        "\n",
        "# 과거 28일의 변화를 보고 미래 28일을 예측\n",
        "x_data = []\n",
        "y_data = []\n",
        "for i in range(data.shape[0]-window_size-future_size):\n",
        "    x = data.iloc[i:i+window_size, 1:].to_numpy()\n",
        "    y = data.iloc[i+window_size:i+window_size+future_size, 3::2].to_numpy()\n",
        "    y_0 = np.zeros([1, y.shape[1]]) # 디코더 첫 입력값 추가\n",
        "    x_data.append(x)\n",
        "    y_data.append(np.concatenate([y_0, y], axis=0))\n",
        "x_data = np.array(x_data)\n",
        "y_data = np.array(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsRRnIn1Xgt0"
      },
      "source": [
        "x_data.shape, y_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwhpCoNXXgt0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEhSuLV_Xgt0"
      },
      "source": [
        "train_test_split = 1\n",
        "x_train = x_data[:-train_test_split-future_size]\n",
        "y_train = y_data[:-train_test_split-future_size]\n",
        "x_val = x_data[-train_test_split:]\n",
        "y_val = y_data[-train_test_split:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k1AeUUIXgt0"
      },
      "source": [
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qggD-7X3Xgt1"
      },
      "source": [
        "# 데이터셋 정의\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.decoder_input = decoder_input\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.encoder_input)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            'encoder_input' : torch.tensor(self.encoder_input[i], dtype=torch.float32),\n",
        "            'decoder_input' : torch.tensor(self.decoder_input[i], dtype=torch.float32)\n",
        "        }\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ1UFMokXgt1"
      },
      "source": [
        "train_dataset = CustomDataset(x_train, y_train)\n",
        "val_dataset = CustomDataset(x_val, y_val)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
        "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=False)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYnowdJXgt2"
      },
      "source": [
        "sample_batch = next(iter(train_dataloader))\n",
        "sample_batch['encoder_input'].shape, sample_batch['decoder_input'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOzM_EwpXgt2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYmcdS8lXgt2"
      },
      "source": [
        "# 모델\n",
        "# 어텐션이 적용된 seq2seq모델\n",
        "\n",
        "# 인코더와 디코더는 GRU 사용\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.rnn = nn.GRU(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp_seq):\n",
        "        inp_seq = inp_seq.permute(1,0,2)\n",
        "        outputs, hidden = self.rnn(inp_seq)\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIc-7pjiXgt3"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, dec_output_dim, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = nn.Linear(dec_output_dim, units)\n",
        "        self.W2 = nn.Linear(dec_output_dim, units)\n",
        "        self.V = nn.Linear(dec_output_dim, 1)\n",
        "\n",
        "    def forward(self, hidden, enc_output):\n",
        "        query_with_time_axis = hidden.unsqueeze(1)\n",
        "        \n",
        "        score = self.V(torch.tanh(self.W1(query_with_time_axis) + self.W2(enc_output)))\n",
        "        \n",
        "        attention_weights = torch.softmax(score, axis=1)\n",
        "        \n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufLCSNcBXgt3"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dec_feature_size, encoder_hidden_dim, output_dim, decoder_hidden_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.layer = nn.Linear(dec_feature_size, encoder_hidden_dim)\n",
        "        self.rnn = nn.GRU(encoder_hidden_dim*2, decoder_hidden_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, enc_output, dec_input, hidden):\n",
        "        dec_input = self.layer(dec_input)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        dec_input = torch.cat([torch.sum(context_vector, dim=0), dec_input], dim=1)\n",
        "        dec_input = dec_input.unsqueeze(0)\n",
        "        \n",
        "        output, hidden = self.rnn(dec_input, hidden)\n",
        "\n",
        "        prediction = self.fc_out(output.sum(0))\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLlLAJlzXgt4"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, attention):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, encoder_input, decoder_input, teacher_forcing=False):\n",
        "        batch_size = decoder_input.size(0)\n",
        "        trg_len = decoder_input.size(1)\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, trg_len-1, self.decoder.output_dim).to(device)\n",
        "        enc_output, hidden = self.encoder(encoder_input)\n",
        "        \n",
        "        dec_input = decoder_input[:, 0]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(enc_output, dec_input, hidden)\n",
        "            outputs[:, t-1] = output\n",
        "            if teacher_forcing == True:\n",
        "                dec_input = decoder_input[:, t]\n",
        "            else:\n",
        "                dec_input = output\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw5DthMPXgt4"
      },
      "source": [
        "encoder = Encoder(input_dim=x_data.shape[-1], hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout)\n",
        "attention = BahdanauAttention(dec_output_dim=hidden_dim, units=hidden_dim)\n",
        "decoder = Decoder(\n",
        "    dec_feature_size=target_n, encoder_hidden_dim=hidden_dim, output_dim=target_n,\n",
        "    decoder_hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout,\n",
        "    attention = attention\n",
        ")\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, attention)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FmdyUAdXgt4"
      },
      "source": [
        "# 평가 함수\n",
        "# 평가를 위한 NMAE 정의b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX18RJk-Xgt5"
      },
      "source": [
        "def my_custom_metric(pred, true):\n",
        "    pred = pred[:, [6, 13, 27]]\n",
        "    true = true[:, [6, 13, 27]]\n",
        "    target = torch.where(true!=0)\n",
        "    true = true[target]\n",
        "    pred = pred[target]\n",
        "    score = torch.mean(torch.abs((true-pred))/(true))\n",
        "    \n",
        "    return score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bVwnCJTXgt5"
      },
      "source": [
        "# 옵티마이저 및 손실함수\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.L1Loss() # mae\n",
        "custom_metric = my_custom_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXoadWEgXgt6"
      },
      "source": [
        "# 학습 정의\n",
        "def train_step(batch_item, epoch, batch, training, teacher_forcing):\n",
        "    encoder_input = batch_item['encoder_input'].to(device)\n",
        "    decoder_input = batch_item['decoder_input'].to(device)\n",
        "    if training is True:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(encoder_input, decoder_input, teacher_forcing)\n",
        "            loss = criterion(output, decoder_input[:,1:])\n",
        "            score = custom_metric(output, decoder_input[:,1:])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        return loss, score\n",
        "    else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(encoder_input, decoder_input, False)\n",
        "            loss = criterion(output, decoder_input[:,1:])\n",
        "            score = custom_metric(output, decoder_input[:,1:])\n",
        "        return loss, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqtM72e5Xgt6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONoXfa-_Xgt6"
      },
      "source": [
        "# 학습\n",
        "loss_plot, val_loss_plot = [], []\n",
        "score_plot, val_score_plot = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss, total_val_loss = 0, 0\n",
        "    total_score, total_val_score = 0, 0\n",
        "    \n",
        "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
        "    training = True\n",
        "    for batch, batch_item in tqdm_dataset:\n",
        "        batch_loss, batch_score = train_step(batch_item, epoch, batch, training, teacher_forcing)\n",
        "        total_loss += batch_loss\n",
        "        total_score += batch_score\n",
        "        \n",
        "        tqdm_dataset.set_postfix({\n",
        "            'Epoch': epoch + 1,\n",
        "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
        "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
        "            'Score': '{:06f}'.format(batch_score.item()),\n",
        "            'Total Score' : '{:06f}'.format(total_score/(batch+1)),\n",
        "        })\n",
        "    loss_plot.append(total_loss/(batch+1))\n",
        "    score_plot.append(total_score/(batch+1))\n",
        "    \n",
        "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
        "    training = False\n",
        "    for batch, batch_item in tqdm_dataset:\n",
        "        batch_loss, batch_val_score = train_step(batch_item, epoch, batch, training, teacher_forcing)\n",
        "        total_val_loss += batch_loss\n",
        "        total_val_score += batch_val_score\n",
        "        \n",
        "        tqdm_dataset.set_postfix({\n",
        "            'Epoch': epoch + 1,\n",
        "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
        "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
        "            'Val Score': '{:06f}'.format(batch_val_score.item()),\n",
        "            'Total Val Score' : '{:06f}'.format(total_val_score/(batch+1)),\n",
        "        })\n",
        "    val_loss_plot.append(total_val_loss/(batch+1))\n",
        "    val_score_plot.append(total_val_score/(batch+1))\n",
        "    \n",
        "    if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
        "        torch.save(model, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fim3bZYPXgt6"
      },
      "source": [
        "# 학습 결과\n",
        "plt.plot(loss_plot, label='train_loss')\n",
        "plt.plot(val_loss_plot, label='val_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss(mae)')\n",
        "plt.title('loss_plot')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(score_plot, label='train_score')\n",
        "plt.plot(val_score_plot, label='val_score')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score(nmae)')\n",
        "plt.title('score_plot')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVH3CIoAXgt6"
      },
      "source": [
        "# 베스트모델 복원\n",
        "\n",
        "model = torch.load(save_path)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ODsgxxyXgt7"
      },
      "source": [
        "# 추론 및 제출\n",
        "\n",
        "def predict(encoder_input):\n",
        "    model.train()\n",
        "    encoder_input = encoder_input.to(device)\n",
        "    decoder_input = torch.zeros([1, future_size+1, target_n], dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(encoder_input, decoder_input, False)\n",
        "    return output.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTnPoau2Xgt7"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/멀캠_장보고/원본데이터/농산물_가격예측_AI 경진대회/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqW454WwXgt7"
      },
      "source": [
        "public_date_list = submission[submission['예측대상일자'].str.contains('2020')]['예측대상일자'].str.split('+').str[0].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLvaWmjMXgt7"
      },
      "source": [
        "# 추론\n",
        "# 일자별 추론을 진행합니다.\n",
        "\n",
        "# data leakage가 발생하지 않도록 주의합니다.\n",
        "outputs = []\n",
        "troch_norm = torch.tensor(norm.to_numpy()[2::2])\n",
        "for date in public_date_list:\n",
        "    test_df = pd.read_csv(f'/content/drive/MyDrive/멀캠_장보고/원본데이터/농산물_가격예측_AI 경진대회/public_data/test_files/test_{date}.csv')\n",
        "    data = pd.read_csv('/content/drive/MyDrive/멀캠_장보고/원본데이터/농산물_가격예측_AI 경진대회/public_data/train.csv')\n",
        "    data = pd.concat([data, test_df]).iloc[-window_size:]\n",
        "    \n",
        "    week_day_map = {}\n",
        "    for i, d in enumerate(data['요일'].unique()):\n",
        "        week_day_map[d] = i\n",
        "    data['요일'] = data['요일'].map(week_day_map)\n",
        "    data = data.iloc[:,1:]/norm\n",
        "    \n",
        "    encoder_input = torch.tensor(data.to_numpy(), dtype=torch.float32)\n",
        "    encoder_input = encoder_input.unsqueeze(0)\n",
        "    output = predict(encoder_input)*troch_norm\n",
        "    \n",
        "    idx = submission[submission['예측대상일자'].str.contains(date)].index\n",
        "    submission.loc[idx, '배추_가격(원/kg)':] = output[0,[6,13,27]].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjt7Hva6pzHe"
      },
      "source": [
        "다ㅣ;러디ㅏ럳;리더리다ㅓㄷ라ㅣㅓ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0o0U0fRXgt8"
      },
      "source": [
        "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/dacon_baseline.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtDEa8CpXgt8"
      },
      "source": [
        "# from dacon_submit_api import dacon_submit_api \n",
        "\n",
        "# result = dacon_submit_api.post_submission_file(\n",
        "#     'dacon_baseline.csv', \n",
        "#     '개인 Token', \n",
        "#     '235801', \n",
        "#     'DACONIO', \n",
        "#     'DACON_Baseline'\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGWrXDideyIU"
      },
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "# print(os.path.dirname(os.path.realpath(__file__)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K8v-9jGdOsF"
      },
      "source": [
        "!pip install ./mydrive/dacon_submit_api-0.0.4-py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPf2IZxopwK9"
      },
      "source": [
        "//// "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYhl8Z_bdiPW"
      },
      "source": [
        "from dacon_submit_api import dacon_submit_api \n",
        "\n",
        "result = dacon_submit_api.post_submission_file(\n",
        "'/content/dacon_baseline.csv', # 제출 파일 위치(파일생성 위치)\n",
        "'fe64dd315f61470022ae7b84f82d837b9808d4e77ab88d55cb30e814ef58f283', # 개인토큰, 개인 발급 필요함. \n",
        "'235801',    # 대회번호, URL 에 있음.\n",
        "'jsson',     # 팀명, 프로필 대회 팀명 표기되어 있음.\n",
        "'Baseline' ) # 메모(선택사항)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjTrQiJHf3GM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}